---
layout: article
title: "[Dev]Advanced Scraping"
key: 20180919
tags:
  - Dev
  - ML
toc: true
mathjax: true
mathjax_autoNumber: true
published : true
---

# [+] Adavanced Scraping

<!--more-->

## [+]  Requests

### requests module

이번엔 로그인이 필요한 페이지에서 로그인을 하고 해당 데이터를 가져오는 내용이다.
책에는 세션과 쿠키에 대해 설명되어있으나 생략하고 진행한다.

테스트 페이지는 한빛출판네트워크 페이지로 진행하도록 한다. 

```python
#-*-coding:utf-8-*-
import requests, sys, bs4, urlparse
reload(sys)
sys.setdefaultencoding('utf-8')
#Set ID & password

id = 'ID'
passwd = 'PASSWD'

#Session
session=requests.session()

#Login
login_info = {
    'm_id':id,
    'm_passwd':passwd
}

login_proc = 'http://www.hanbit.co.kr/member/login_proc.php'
resp=session.post(login_proc,data=login_info)
resp.raise_for_status()

#Access My page
url_mypage='http://www.hanbit.co.kr/myhanbit/myhanbit.html'
resp=session.get(url_mypage)
resp.raise_for_status()

#Mileage and eCoin
soup=bs4.BeautifulSoup(resp.text,'html.parser')
mile = soup.select_one('.mileage_section1 span').get_text()
ecoin = soup.select_one('.mileage_section2 span').get_text()
print "마일리지 : {}".format(mile)
print "E코인 : {}".format(ecoin)

```

프로그램의 흐름을 살펴보자.
먼저 아이디와 비밀번호를 변수에 저장한다. 그리고 `session()` 함수를 이용하여 세션을 시작한다. 그리고 `login_info` 를 딕셔너리 형태로 파라미터 값을 저장하고, `login_proc` 변수에 로그인 프로세스의 URL을 지정한다. 그 후 `session.post()` 를 이용하여 포스트 방식으로 로그인 프로세스 url에 `login_info` 를 데이터로 전달하고 그 응답을 저장한다.

`session`인스턴스에 `raise_for_status()`의 경우 해당 URL 요청에 대한 응답의 예외처리를 담당한다. 예를 들어 찾을 수 없는 페이지의 경우 '404' 에러로 에러가 발생되었음을 알려준다.

로그인 후 마이페이지에 `session.get()` 을 이용해 get요청을 하고 그 응답을 저장한다.

다음은 `BeautifulSoup`을 이용하여 마이페이지의 내용을 가져오고, 마일리지와 ecoin의 값을 추출하여 출력하여 준다.

위에서 사용한 `requests` 모듈에는 많은 메소드가 존재한다. 흔히 사용하는 GET,POST,PUT,DELETE,HEAD 메소드가 모두 존재한다.

```python
from requests import *
#Get 요청
r = get('http://google.com')

#POST 요청
pdata = {'key':'value','key2':'value2'}
r = post('http://example.com',data=pdata)
```

위의 `r` 변수에 담긴 리턴 값에 존재하는 text와 content 속성을 참조하여 내부 데이터를 확인할 수 있다.

```python
#-*- coding:utf-8 -*-
import sys
from requests import *
reload(sys)
sys.setdefaultencoding('utf-8')

# Data extraction

r = get('http://api.aoikujira.com/time/get.php')
print r.text
print r.content

# Binary Download
r = get('http://wikibook.co.kr/wikibook.png')

try:
    with open('test.png','wb') as f:
        f.write(r.content)
    print '[+] Saved'
except Exception as e:
    print e
```

간단하게 시간을 출력하는 형태와, 그림과 같은 바이너리를 요청하여 저장하는 프로그램이다. 

## [+] Selenium, PhantomJS

### Browser Control

자바스크립트를 이용하는 웹 사이트는 웹 브라우저를 사용하지 않을 경우 정상 동작을 확인하기 어렵다. 이러한 페이지는 `requests` 모듈로 대처할 수 없다. 그렇기 때문에 웹 브라우저를 조작하는 방식이 필요한데 이 때 사용하는 도구가 바로 `Selenium` 이다.

이번엔 우분투 리눅스를 이용한다. 설치해야 할 목록 및 명령어는 다음과 같다.

```shell
# selenium install
$ pip install selenium

# phantomJS download
$ wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2
```

`phantomJS`는 압축을 해제한 후 환경변수 지정이든 `/usr/local/bin` 이든 바이너리를 복사해서 사용한다.

```python
# -*- coding:utf-8 -*-
from selenium import webdriver

url ='https://www.naver.com'

#PhantomJS Driver
browser = webdriver.PhantomJS()
browser.implicitly_wait(3)

#URL read
browser.get(url)

#Capture browser
browser.save_screenshot('website.png')
browser.quit()
```

오오오............엄청 신기하다. 브라우저를 열은 적도 없건만 가상의 브라우저를 열어 캡쳐까지 떠서 파일로 저장해준다. 실제 캡쳐화면을 보면 네이버 메인페이지가 나오는 것을 확인할 수 있다.

그리고 이전에 `requests` 모듈을 이용하였을 때 네이버와 같이 보안 요소가 존재하는 페이지는 로그인을 하지 못했었다. 그러나 `selenium`은 가능하다!!!!!!!!!! 확인해본다..

```python
# -*-coding:utf-8-*-
from selenium import webdriver

id = 'id'
passwd ="passwd"

browser=webdriver.PhantomJS()
browser.implicitly_wait(3)

url_login='https://nid.naver.com/nidlogin.login'
browser.get(url_login)
print '[+] 로그인 페이지에 접근'
#browser.save_screenshot('website1.png')
e = browser.find_element_by_id('id')
e.clear()
e.send_keys(id)
e = browser.find_element_by_id('pw')
e.clear()
e.send_keys(passwd)

form = browser.find_element_by_css_selector('input.btn_global[type=submit]')
print '[+] 로그인 버튼 클릭'
form.submit()
#browser.save_screenshot('website2.png')
browser.get('https://order.pay.naver.com/home?tabMenu=SHOPPING')
#browser.save_screenshot('website3.png')
products =browser.find_elements_by_css_selector('.p_info span')
print products
for product in products:
    print '-', product.text
```

음 아주 편리한데 네이버에서 캡챠코드를 이용해서........로그인이 불가능하다. 하 슬프다. 

**계속...**

# [+] Reference

1. <a href="http://wikibook.co.kr/python-machine-learning/">*"파이썬을 이용한 머신러닝"*</a>