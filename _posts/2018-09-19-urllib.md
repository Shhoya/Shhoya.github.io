---
layout: article
title: "[Dev]Web Scraping[2]"
key: 20180919
tags:
  - Python
  - Dev
  - ML
toc: true
mathjax: true
mathjax_autoNumber: true
published : true
---

# [+] Web Scraping

<!--more-->

## [+] download from link

### Use urljoin() method

```python
#-*-coding:utf-8-*-
import sys
from urlparse import urljoin
reload(sys)
sys.setdefaultencoding('utf-8')

base = "http://example.com/html/a.html"
print urljoin(base,'b.html')
print urljoin(base,'sub/c.html')
print urljoin(base,'../index.html')
print urljoin(base,'../img/hoge.png')
print urljoin(base,'../css/hoge.css')
```

```
http://example.com/html/b.html
http://example.com/html/sub/c.html
http://example.com/index.html
http://example.com/img/hoge.png
http://example.com/css/hoge.css
```

결과를 확인하면 base url을 기반으로 상대 경로를 절대경로로 변환하여 준다. 이 때 사용하는 메소드가 바로 urljoin() 이다.

```python
usage : 
import urlparse import urljoin
urljoin(base,path)
```

그러나 만약 path 인자가 'http://' 등 절대경로를 암시하는 문자열이 들어가면 앞에 'base' 인자를 무시하게 된다.

```python
#-*-coding:utf-8-*-
import sys
from urlparse import urljoin
reload(sys)
sys.setdefaultencoding('utf-8')

base = "http://example.com/html/a.html"

print urljoin(base,'/hoge.html')
print urljoin(base,'http://otherExample.com/wiki')
print urljoin(base,'//anotherExample.com/test')
```

```
http://example.com/hoge.html
http://otherExample.com/wiki
http://anotherExample.com/test
```



### Recursive HTML

HTML 링크 구조에 관한 얘기가 책에 나와있다. 간단히 설명하면 a.html -> b.html -> c.html 이런 구조가 있을 때 a 에서 c까지 이동하며 페이를 모두 다운로드 받지 않으면 링크가 잘리는 문제가 발생하기 때문에 a~c까지 모두 분석을 해야 한다. 이런 말이다. 

그럴 때 사용 가능한 방법이 재귀처리를 하는 재귀함수를 만든다는 이야기 같다.

순서는 다음과 같다.

1. HTML 분석
2. 링크 추출
3. 각 링크 대상 처리
4. 파일 다운로드
5. 파일이 html 이면 1로 돌아가 다시 실행

다음 코드는 살짝 빡신데, 파이썬 공식 라이브러리 내 문서를 모두 다운로드 받는 코드이다.

```python
#-*-coding:utf-8-*-
# 파이썬 매뉴얼을 재귀적으로 다운받는 프로그램
# 모듈 읽어 들이기 --- (※1)
from bs4 import BeautifulSoup
from urllib import *
from urlparse import *
from os import makedirs
import os.path, time, re
import sys,io
reload(sys)
sys.setdefaultencoding('utf-8')
# 이미 처리한 파일인지 확인하기 위한 변수 --- (※2)
proc_files = {}
# HTML 내부에 있는 링크를 추출하는 함수 --- (※3)
def enum_links(html, base):
    soup = BeautifulSoup(html, "html.parser")
    links = soup.select("link[rel='stylesheet']") # CSS
    links += soup.select("a[href]") # 링크
    result = []
    # href 속성을 추출하고, 링크를 절대 경로로 변환 --- (※4)
    for a in links:
        href = a.attrs['href']
        url = urljoin(base, href)
        result.append(url)
    return result
# 파일을 다운받고 저장하는 함수 --- (※5)
def download_file(url):
    o = urlparse(url)
    savepath = "./" + o.netloc + o.path
    if re.search(r"/$", savepath): # 폴더라면 index.html
        savepath += "index.html"
    savedir = os.path.dirname(savepath)
    # 모두 다운됐는지 확인
    if os.path.exists(savepath): return savepath
    # 다운받을 폴더 생성
    if not os.path.exists(savedir):
        print("mkdir=", savedir)
        makedirs(savedir)
    # 파일 다운받기 --- (※6)
    try:
        print("download=", url)
        urlretrieve(url, savepath)
        time.sleep(1) # 1초 휴식 --- (※7)
        return savepath
    except:
        print("다운 실패: ", url)
        return None
# HTML을 분석하고 다운받는 함수 --- (※8)
def analyze_html(url, root_url):
    savepath = download_file(url)
    if savepath is None: return
    if savepath in proc_files: return # 이미 처리됐다면 실행하지 않음 --- (※9)
    proc_files[savepath] = True
    print("analyze_html=", url)
    # 링크 추출 --- (※10)
    html = io.open(savepath, "r", encoding='utf-8').read()
    links = enum_links(html, url)
    for link_url in links:
        # 링크가 루트 이외의 경로를 나타낸다면 무시 --- (※11)
        if link_url.find(root_url) != 0:
            if not re.search(r".css$", link_url): continue
        # HTML이라면
        if re.search(r".(html|htm)$", link_url):
            # 재귀적으로 HTML 파일 분석하기
            analyze_html(link_url, root_url)
            continue
        # 기타 파일
        download_file(link_url)
if __name__ == "__main__":
    # URL에 있는 모든 것 다운받기 --- (※12)
    url = "https://docs.python.org/2.7/library/"
    analyze_html(url, url)
```

나는 python 2.x 를 사용하므로 위와 같이 코드를 살짝 수정하였다.
잘돌아간다.. 정말다받는다..; 참고로 쉘에서 바로 실행해야 다운로드 받아진다. atom 과 같은 에디터에서는 다운로드 받아지지 않는다.

분석 내용이다.
**※1** : 사용되는 모듈을 적재한다. 데이터를 받기 위한 urllib, 분석을 위한 urlparse, 폴더 생성을 위한 os, 경로 및 슬립을 위한 os.path, time, 그리고 정규표현식을 위한 re 모듈이다.

**※2** : 전역 변수인 'proc_files'를 초기화 한다. 이는 이미 분석한 파일인지 분별하기 위함이다.

**※3** : ```enum_links()``` 함수에서 HTML을 분석하고 링크를 추출한다. a 태그로 링크, link 태그로 경로를 찾으며 `BeautifulSoup`의 `select`를 이용한다.

**※4** : 링크 태그의 href 속성에 있는 URL을 추출하고 절대 경로로 변환한다. 위에서 잠깐 해봤던 urljoin을 이용한다.

**※5** : 파일을 다운받고 저장하는 `download_file` 함수이다. 해당 함수 내 `try` 문을 통해 실제 다운로드를 받는다.

**※6** : `urlretrieve` 를 이용해 실제 다운로드를 받는다.

**※7** : 웹 서버에 부하를 주지 않기 위해 `sleep`함수를 이용한다.

**※8** : `analyze_html()` 함수에서는 HTMl 파일을 분석하고 링크에 있는 것을 다운받는다.

**※9** : 같은 파일을 반복하지 않도록 한다.

**※10** : 링크를 추출한다.

**※11** : 링크 대상을 확인 후, 해당 사이트가 아닌 경우 다운로드 하지 않도록 한다. 그러나 css로 인해 깨지는 것을 방지하기 위해 css는 예외로 모두 다운로드 받는다.

**※12** : 어떤 URL에서 다운로드 할지 지정한다. 



여기까지가 기본 크롤링 및 스크레이핑 이었다.
오늘 배운 것은 뭔가 좀 약간... 갑자기 난이도가 급상승한 느낌?!

천천히 빡세게 하자~

# [+] Reference

1. <a href="http://wikibook.co.kr/python-machine-learning/">*"파이썬을 이용한 머신러닝"*</a>

