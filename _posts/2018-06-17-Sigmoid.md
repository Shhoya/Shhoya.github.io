---
layout: article
title: "[ML]Sigmoid Function"
key: 20180617
tags:
  - Dev
  - ML
  - Neural
  - Python
toc: true
mathjax: true
mathjax_autoNumber: true
---

# [+] Sigmoid Function

## [+] Function implementation

### python

시그모이드 함수에 대해서는 이전 <a href="https://shhoya.github.io/2018/06/05/Activation-Function.html">활성화 함수</a>에 대해 설명할 때 간단히 설명한 적이 있다. 이번엔 파이썬으로 간략하게 시그모이드 함수가 어떻게 생겼는지 알아본다.

먼저 파이썬 코드는 다음과 같다.

```python
import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
    return 1 / (1+np.exp(-x))

x = np.arange(-5.0,5.0,0.1)
y = sigmoid(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1)
plt.show()
```

실행하면 다음과 같은 그래프를 볼 수 있다.
![sigmoid](https://github.com/Shhoya/Shhoya.github.io/blob/master/assets/images/task/sigmoid.png?raw=true "sigmoid"){:.border}

그래프를 보면 부드럽게 S자 모양을 볼 수 있다.



### Different

그렇다면 계단함수와 시그모이드 함수의 차이에 대해 알아본다.

```python
import numpy as np
import matplotlib.pylab as plt

def step_func(x):
    return np.array(x >0,dtype=np.int)


def sigmoid(x):
    return 1 / (1+np.exp(-x))

x = np.arange(-5.0,5.0,0.1)
y1 = sigmoid(x)
y2 = step_func(x)
plt.plot(x,y1,label="sigmoid")
plt.plot(x,y2,linestyle="--",label="step")
plt.ylim(-0.1,1.1)
plt.show()
```

위와 같이 실행하여 그래프를 보면 다음과 같이 차이를 볼 수 있다.
![sigmoid](https://github.com/Shhoya/Shhoya.github.io/blob/master/assets/images/task/sigmoid2.png?raw=true "sigmoid"){:.border}

시그모이드 함수는 곡선이며, 입력에 따라 출력이 계속해서 변화한다, 계단함수는 0을 경계로 출력이 극단적으로 변화한다.
바로 이 곡선, 매끄러움 때문에 신경망 학습에서 아주 중요한 역할을 하게 된다. 또한 둘의 공통점은 입력이 작을 때 출력이 0에 가깝거나 0이고, 입력이 커지면 출력이 1에 아까워지거나 1이 된다. 또한 입력이 아무리 커도 출력은 0과 1 사이라는 것이 공통점이다.

## [+] Example

실제 시그모이드 함수를 채택하는 여러가지 함수들에 대한 예제가 있어 남겨 놓는다.

+ Logistic Function (Normal)

  $$h(x) ={ 1 \over 1+exp^{-x}}$$

+ Hyperbolic tangent

  $$h(x) = { e^x - e^{-x} \over e^x+e^{-x}}$$

+ Gudermannian Function

  $$h(x) = arctan $$ $$x$$



# [+] Reference

1. <a href="http://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198">*"밑바닥부터 시작하는 딥러닝"*</a>
2. <a href="https://en.wikipedia.org/wiki/Sigmoid_function">*"Wiki Sigmoid Function"*</a>

